{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "tokenizer = RegexpTokenizer('\\/|^\\.|\\.$|,|;|\\(|\\)|^\\-|\\-$|:|;', gaps=True)\n",
    "import spacy\n",
    "import nltk\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
    "import codecs\n",
    "import re\n",
    "import yake\n",
    "import spacy_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# load default skills data base\n",
    "from skillNer.general_params import SKILL_DB\n",
    "# import skill extractor\n",
    "from skillNer.skill_extractor_class import SkillExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tokens):\n",
    "        excluded = set(string.punctuation)\n",
    "        excluded_2 = set(['year', 'years', 'etc', \"#\", \"&\"])\n",
    "        #import re\n",
    "        url_pattern = re.compile(\"^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\")\n",
    "        span_pattern = re.compile(r\"/?span|class=|''|hl\")\n",
    "        output = []\n",
    "        for token in tokens :\n",
    "            if token not in excluded and token not in excluded_2 and not url_pattern.match(token) and not span_pattern.match(token):\n",
    "                for word in tokenizer.tokenize(token) :\n",
    "                    if len(word) >= 2 :\n",
    "                        output.append(word)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_folder = 'D:/NYU/NLP-GA/final project/resumes_corpus'\n",
    "\n",
    "files = os.listdir(target_folder)\n",
    "\n",
    "selected_files = [file for file in files if ('00001.txt' <= file <= '00100.txt' and file[-3:] =='txt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xujy\\anaconda3\\envs\\pytorch2-env\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.2.4 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "#model_2\n",
    "nlp2 = spacy.load(\"D:/NYU/NLP-GA/final project/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading full_matcher ...\n",
      "loading abv_matcher ...\n",
      "loading full_uni_matcher ...\n",
      "loading low_form_matcher ...\n",
      "loading token_matcher ...\n"
     ]
    }
   ],
   "source": [
    "#model_1\n",
    "skill_extractor = SkillExtractor(nlp, SKILL_DB, PhraseMatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00001.txt', '00002.txt', '00003.txt', '00004.txt', '00005.txt', '00006.txt', '00007.txt', '00008.txt', '00009.txt', '00010.txt', '00011.txt', '00012.txt', '00013.txt', '00014.txt', '00015.txt', '00016.txt', '00017.txt', '00018.txt', '00019.txt', '00020.txt', '00021.txt', '00022.txt', '00023.txt', '00024.txt', '00025.txt', '00026.txt', '00027.txt', '00028.txt', '00029.txt', '00030.txt', '00031.txt', '00032.txt', '00033.txt', '00034.txt', '00035.txt', '00036.txt', '00037.txt', '00038.txt', '00039.txt', '00040.txt', '00041.txt', '00042.txt', '00043.txt', '00044.txt', '00045.txt', '00046.txt', '00047.txt', '00048.txt', '00049.txt', '00050.txt', '00051.txt', '00052.txt', '00053.txt', '00054.txt', '00055.txt', '00056.txt', '00057.txt', '00058.txt', '00059.txt', '00060.txt', '00061.txt', '00062.txt', '00063.txt', '00064.txt', '00065.txt', '00066.txt', '00067.txt', '00068.txt', '00069.txt', '00070.txt', '00071.txt', '00072.txt', '00073.txt', '00074.txt', '00075.txt', '00076.txt', '00077.txt', '00078.txt', '00079.txt', '00080.txt', '00081.txt', '00082.txt', '00083.txt', '00084.txt', '00085.txt', '00086.txt', '00087.txt', '00088.txt', '00089.txt', '00090.txt', '00091.txt', '00092.txt', '00093.txt', '00094.txt', '00095.txt', '00096.txt', '00097.txt', '00098.txt', '00099.txt', '00100.txt']\n"
     ]
    }
   ],
   "source": [
    "print(selected_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xujy\\anaconda3\\envs\\pytorch2-env\\lib\\site-packages\\skillNer\\utils.py:99: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  vec_similarity = token1.similarity(token2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001.txt done\n",
      "00002.txt done\n",
      "00003.txt done\n",
      "00004.txt done\n",
      "00005.txt done\n",
      "00006.txt done\n",
      "00007.txt done\n",
      "00008.txt done\n",
      "00009.txt done\n",
      "00010.txt done\n",
      "00011.txt done\n",
      "00012.txt done\n",
      "00013.txt done\n",
      "00014.txt done\n",
      "00015.txt done\n",
      "00016.txt done\n",
      "00017.txt done\n",
      "00018.txt done\n",
      "00019.txt done\n",
      "00020.txt done\n",
      "00021.txt done\n",
      "00022.txt done\n",
      "00023.txt done\n",
      "00024.txt done\n",
      "00025.txt done\n",
      "00026.txt done\n",
      "00027.txt done\n",
      "00028.txt done\n",
      "00029.txt done\n",
      "00030.txt done\n",
      "00031.txt done\n",
      "00032.txt done\n",
      "00033.txt done\n",
      "00034.txt done\n",
      "00035.txt done\n",
      "00036.txt done\n",
      "00037.txt done\n",
      "00038.txt done\n",
      "00039.txt done\n",
      "00040.txt done\n",
      "00041.txt done\n",
      "00042.txt done\n",
      "00043.txt done\n",
      "00044.txt done\n",
      "00045.txt done\n",
      "00046.txt done\n",
      "00047.txt done\n",
      "00048.txt done\n",
      "00049.txt done\n",
      "00050.txt done\n",
      "00051.txt done\n",
      "00052.txt done\n",
      "00053.txt done\n",
      "00054.txt done\n",
      "00055.txt done\n",
      "00056.txt done\n",
      "00057.txt done\n",
      "00058.txt done\n",
      "00059.txt done\n",
      "00060.txt done\n",
      "00061.txt done\n",
      "00062.txt done\n",
      "00063.txt done\n",
      "00064.txt done\n",
      "00065.txt done\n",
      "00066.txt done\n",
      "00067.txt done\n",
      "00068.txt done\n",
      "00069.txt done\n",
      "00070.txt done\n",
      "00071.txt done\n",
      "00072.txt done\n",
      "00073.txt done\n",
      "00074.txt done\n",
      "00075.txt done\n",
      "00076.txt done\n",
      "00077.txt done\n",
      "00078.txt done\n",
      "00079.txt done\n",
      "00080.txt done\n",
      "00081.txt done\n",
      "00082.txt done\n",
      "00083.txt done\n",
      "00084.txt done\n",
      "00085.txt done\n",
      "00086.txt done\n",
      "00087.txt done\n",
      "00088.txt done\n",
      "00089.txt done\n",
      "00090.txt done\n",
      "00091.txt done\n",
      "00092.txt done\n",
      "00093.txt done\n",
      "00094.txt done\n",
      "00095.txt done\n",
      "00096.txt done\n",
      "00097.txt done\n",
      "00098.txt done\n",
      "00099.txt done\n",
      "00100.txt done\n"
     ]
    }
   ],
   "source": [
    "for file_name in selected_files:\n",
    "    f = codecs.open(target_folder+'/'+file_name, \"rU\", encoding='utf-8', errors='ignore')\n",
    "    f=f.read()\n",
    "    sentences_seg = nltk.sent_tokenize(f)\n",
    "    sentences = [ ' '.join([token.lemma_ for token in nlp(\" \".join(clean(nltk.tokenize.word_tokenize(sent))))])+'.\\n' for sent in sentences_seg ]\n",
    "    with open (\"D:/NYU/NLP-GA/final project/extracted_resume_model_2\"+'/'+file_name, \"w\") as fi:\n",
    "        for doc in nlp2.pipe(sentences, disable=[\"tagger\", \"parser\"]):\n",
    "            for ent in doc.ents:\n",
    "                text_name = re.sub('[^A-Za-z0-9]+', ' ', ent.text).strip()\n",
    "                fi.write(text_name + '\\n')\n",
    "    fi.close()\n",
    " \n",
    "    with open (\"D:/NYU/NLP-GA/final project/extracted_resume_model_1\"+'/'+file_name, \"w\") as fi:\n",
    "        content = ' '.join(sentences)\n",
    "        annotations = skill_extractor.annotate(content)\n",
    "        for i in annotations['results']['full_matches']:\n",
    "            fi.write(i['doc_node_value']+'\\n')\n",
    "    fi.close()   \n",
    "    print(file_name + \" done\")\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
